{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce402168",
   "metadata": {},
   "source": [
    "# Assignment : AttenTion Based Model and Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a5e5ae",
   "metadata": {},
   "source": [
    "Q1. What is BERT and how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2ff717",
   "metadata": {},
   "source": [
    "Answer : BERT (Bidirectional Encoder Representations from Transformers) is a pre-trained language model developed by Google. It uses a multi-layer bidirectional transformer encoder to generate contextualized representations of words in a sentence. BERT is pre-trained on a large corpus of text and then fine-tuned for specific tasks, achieving state-of-the-art results in many natural language processing (NLP) tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59184898",
   "metadata": {},
   "source": [
    "Q2. What are the main advantages of using the attention mechanism in neural networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c469797",
   "metadata": {},
   "source": [
    "Answer : The attention mechanism allows a model to focus on the most relevant parts of the input data when generating output. This improves performance in tasks involving sequential data, such as machine translation or text summarization, by enabling the model to capture long-range dependencies and contextual relationships more effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0096e031",
   "metadata": {},
   "source": [
    "Q3. How does the self-attention mechanism differ from traditional attention mechanisms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9da16e",
   "metadata": {},
   "source": [
    "Answer : Self-attention, also known as intra-attention, allows a model to attend to different parts of the same input sequence, weighing their importance. Traditional attention mechanisms typically involve attending to a separate input sequence (e.g., the output of an encoder in a sequence-to-sequence model). Self-attention is a key component of the Transformer architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccf4ca4",
   "metadata": {},
   "source": [
    "Q4. What is the role of the decoder in a Seq2Seq model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ce107b",
   "metadata": {},
   "source": [
    "Answer : In a sequence-to-sequence (Seq2Seq) model, the decoder generates output sequences based on the encoded input sequence. It uses the information encoded by the encoder to produce a sequence of outputs, often one token at a time, until a stopping criterion is met."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52aea5a",
   "metadata": {},
   "source": [
    "Q5. What is the difference between GPT-2 and BERT models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ae5122",
   "metadata": {},
   "source": [
    "Answer : GPT-2 (Generative Pre-trained Transformer 2) is a unidirectional model primarily designed for text generation tasks. It uses a left-to-right transformer architecture to predict the next word in a sequence. BERT, on the other hand, is bidirectional and is pre-trained to predict words in a sentence based on both left and right contexts, making it more suitable for tasks requiring understanding of the entire sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a43d84c",
   "metadata": {},
   "source": [
    "Q6. Why is the Transformer model considered more efficient than RNNs and LSTMs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9082220",
   "metadata": {},
   "source": [
    "Answer : Transformers are more efficient than Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks for many tasks because they can process input sequences in parallel, unlike RNNs and LSTMs, which process sequences sequentially. This parallelization significantly speeds up training times for large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4882530e",
   "metadata": {},
   "source": [
    "Q7. Explain how the attention mechanism works in a Transformer model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b9f6ec",
   "metadata": {},
   "source": [
    "Answer : In a Transformer model, the attention mechanism computes the weighted sum of the input elements (e.g., words in a sentence) based on their relevance to each other. This is done through self-attention, where the model calculates attention scores by comparing each element with all others and uses these scores to compute a weighted sum, capturing complex dependencies within the input sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfe957c",
   "metadata": {},
   "source": [
    "Q8. What is the difference between an encoder and a decoder in a Seq2Seq model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4901997f",
   "metadata": {},
   "source": [
    "Answer : The encoder in a Seq2Seq model processes the input sequence and generates a continuous representation of the input. The decoder then uses this representation to generate the output sequence. The key difference is that the encoder focuses on understanding the input, while the decoder focuses on generating the output based on that understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398e152c",
   "metadata": {},
   "source": [
    "Q9. What is the primary purpose of using the self-attention mechanism in transformers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf65605",
   "metadata": {},
   "source": [
    "Answer : The primary purpose of self-attention in transformers is to allow the model to weigh the importance of different parts of the input sequence relative to each other. This enables the model to capture complex, long-range dependencies within the sequence, which is crucial for tasks like machine translation, text summarization, and question answering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4ff137",
   "metadata": {},
   "source": [
    "Q10. How does the GPT-2 model generate text?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5257f5ee",
   "metadata": {},
   "source": [
    "Answer : GPT-2 generates text by predicting one token at a time, starting from a given prompt. It uses its transformer-based architecture to consider the context of the previous tokens and selects the next token based on the probability distribution learned during training. This process continues until a stopping criterion is met or a desired length is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aad9a27",
   "metadata": {},
   "source": [
    "Q11. Explain the concept of “fine-tuning” in BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca88458b",
   "metadata": {},
   "source": [
    "Answer : Fine-tuning in BERT refers to the process of adjusting the pre-trained model's weights to fit a specific task. After pre-training on a large corpus, BERT is fine-tuned on a smaller, task-specific dataset to adapt its learned representations to the nuances of the task at hand, such as sentiment analysis or question answering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e5ed81",
   "metadata": {},
   "source": [
    "Q12. What is the main difference between the encoder-decoder architecture and a simple neural network?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8fff9f",
   "metadata": {},
   "source": [
    "Answer : The main difference is that the encoder-decoder architecture is designed to handle sequential data and complex transformations between input and output sequences. A simple neural network, in contrast, typically processes fixed-size inputs and outputs and is not inherently designed for sequence-to-sequence tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24da1a5",
   "metadata": {},
   "source": [
    "Q13. How does the attention mechanism handle long-range dependencies in sequences?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7874c5c",
   "metadata": {},
   "source": [
    "Answer : The attention mechanism handles long-range dependencies by allowing the model to directly consider the relationships between all elements in the input sequence, regardless of their distance from each other. This is achieved through self-attention, where each element in the sequence is compared to every other element, enabling the model to capture dependencies that might be far apart."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00dca6f",
   "metadata": {},
   "source": [
    "Q14. What is the core principle behind the Transformer architecture?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec09428d",
   "metadata": {},
   "source": [
    "Answer : The core principle behind the Transformer architecture is self-attention, which enables the model to weigh the importance of different parts of the input sequence relative to each other. This is combined with an encoder-decoder structure and feed-forward neural networks to process sequences in parallel, making the Transformer highly efficient and effective for many NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd3e2e7",
   "metadata": {},
   "source": [
    "Q15. What is the role of the \"position encoding\" in a Transformer model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6836f8c",
   "metadata": {},
   "source": [
    "Answer : Position encoding in a Transformer model adds information about the position of each element in the input sequence to the model's embeddings. Since the Transformer architecture does not inherently capture the order of the sequence (unlike RNNs), position encoding helps the model understand the sequence's structure and the relative positions of its elements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b030ea",
   "metadata": {},
   "source": [
    "Q16. How do Transformers use multiple layers of attention?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdf190f",
   "metadata": {},
   "source": [
    "Answer : Transformers use multiple layers of attention to progressively refine the model's understanding of the input sequence. Each layer applies self-attention and feed-forward processing to the output of the previous layer, allowing the model to capture increasingly complex relationships and dependencies within the sequence. This hierarchical processing enables the model to learn rich and nuanced representations of the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e69b6ed",
   "metadata": {},
   "source": [
    "Q17. What does it mean when a model is described as “autoregressive” like GPT-2?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb30553c",
   "metadata": {},
   "source": [
    "Answer : When a model is described as autoregressive, like GPT-2, it means that the model generates output one element at a time, with each new element depending on the previously generated elements. In the context of text generation, this means that GPT-2 predicts the next word in a sequence based on the words it has already generated, creating a sequence of outputs that are dependent on each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fa7691",
   "metadata": {},
   "source": [
    "Q18. How does BERT's bidirectional training improve its performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f53cca",
   "metadata": {},
   "source": [
    "Answer : BERT's bidirectional training improves its performance by allowing the model to consider both the left and right contexts of each word in a sentence during training. This bidirectional approach enables BERT to capture a more comprehensive understanding of the sentence's meaning and context, leading to better performance on tasks that require understanding the full context of a sentence, such as question answering and sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce4e36a",
   "metadata": {},
   "source": [
    "Q19. What are the advantages of using the Transformer over RNN-based models in NLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7739e8",
   "metadata": {},
   "source": [
    "Answer : The advantages of using the Transformer over RNN-based models in NLP include:\n",
    "1. Parallelization: Transformers can process input sequences in parallel, making them much faster to train than RNNs, which process sequences sequentially.\n",
    "2. Long-range dependencies: The self-attention mechanism in Transformers can capture long-range dependencies more effectively than RNNs.\n",
    "3. Scalability: Transformers can handle longer sequences and larger datasets more efficiently than RNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4add258",
   "metadata": {},
   "source": [
    "Q20. What is the attention mechanism’s impact on the performance of models like BERT and GPT-2?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ecb591",
   "metadata": {},
   "source": [
    "Answer : The attention mechanism has a significant impact on the performance of models like BERT and GPT-2 by enabling them to capture complex dependencies and relationships within input sequences. In BERT, attention helps the model understand the full context of a sentence, while in GPT-2, attention allows the model to generate coherent and contextually appropriate text by considering the relationships between previously generated words. This has led to state-of-the-art results in many NLP tasks for both models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c575d403",
   "metadata": {},
   "source": [
    "# Practical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7eb5667",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "1. How to implement a simple text classification model using LSTM in Keras?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06a29c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HEART\\anaconda3\\envs\\NLP\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6s/step - accuracy: 0.2000 - loss: 1.1089\n",
      "Epoch 2/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 259ms/step - accuracy: 0.4000 - loss: 1.0948\n",
      "Epoch 3/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248ms/step - accuracy: 0.6000 - loss: 1.0873\n",
      "Epoch 4/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 284ms/step - accuracy: 0.8000 - loss: 1.0669\n",
      "Epoch 5/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 313ms/step - accuracy: 0.8000 - loss: 1.0602\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1dab38cea90>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "X = [\n",
    "    \"I love programming in Python\",\n",
    "    \"JavaScript is a versatile language\",\n",
    "    \"Machine learning is fascinating\",\n",
    "    \"I enjoy learning new technologies\",\n",
    "    \"Data science is a growing field\"\n",
    "]\n",
    "\n",
    "y = [0, 1, 2, 0, 2]\n",
    "\n",
    "# Assuming X is your text data and y is your labels\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X)\n",
    "sequences = tokenizer.texts_to_sequences(X)\n",
    "X_padded = pad_sequences(sequences, maxlen=200)\n",
    "\n",
    "# Convert labels to categorical\n",
    "y_categorical = to_categorical(y)\n",
    "\n",
    "# Define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(5000, 100, input_length=200))\n",
    "model.add(LSTM(100, dropout=0.2))\n",
    "model.add(Dense(len(set(y)), activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "model.fit(X_padded, y_categorical, epochs=5, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651e5a3f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "2. How to generate sequences of text using a Recurrent Neural Network (RNN)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cce9cae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HEART\\anaconda3\\envs\\NLP\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6s/step - loss: 1.0009\n",
      "Epoch 2/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 314ms/step - loss: 0.9693\n",
      "Epoch 3/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - loss: 0.9395\n",
      "Epoch 4/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - loss: 0.9116\n",
      "Epoch 5/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 0.8855\n",
      "Epoch 6/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - loss: 0.8611\n",
      "Epoch 7/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.8382\n",
      "Epoch 8/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step - loss: 0.8168\n",
      "Epoch 9/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.7967\n",
      "Epoch 10/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 332ms/step - loss: 0.7779\n",
      "Epoch 11/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 361ms/step - loss: 0.7601\n",
      "Epoch 12/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.7433\n",
      "Epoch 13/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.7273\n",
      "Epoch 14/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 0.7120\n",
      "Epoch 15/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - loss: 0.6973\n",
      "Epoch 16/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - loss: 0.6831\n",
      "Epoch 17/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 0.6692\n",
      "Epoch 18/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 303ms/step - loss: 0.6556\n",
      "Epoch 19/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - loss: 0.6423\n",
      "Epoch 20/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - loss: 0.6290\n",
      "Epoch 21/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 419ms/step - loss: 0.6159\n",
      "Epoch 22/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 296ms/step - loss: 0.6029\n",
      "Epoch 23/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 349ms/step - loss: 0.5899\n",
      "Epoch 24/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 294ms/step - loss: 0.5770\n",
      "Epoch 25/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 0.5641\n",
      "Epoch 26/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 354ms/step - loss: 0.5511\n",
      "Epoch 27/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 0.5382\n",
      "Epoch 28/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - loss: 0.5251\n",
      "Epoch 29/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - loss: 0.5120\n",
      "Epoch 30/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 0.4987\n",
      "Epoch 31/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 332ms/step - loss: 0.4854\n",
      "Epoch 32/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 340ms/step - loss: 0.4718\n",
      "Epoch 33/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step - loss: 0.4581\n",
      "Epoch 34/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - loss: 0.4442\n",
      "Epoch 35/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 309ms/step - loss: 0.4302\n",
      "Epoch 36/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - loss: 0.4161\n",
      "Epoch 37/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 335ms/step - loss: 0.4018\n",
      "Epoch 38/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 334ms/step - loss: 0.3874\n",
      "Epoch 39/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 601ms/step - loss: 0.3729\n",
      "Epoch 40/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - loss: 0.3583\n",
      "Epoch 41/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step - loss: 0.3436\n",
      "Epoch 42/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 0.3288\n",
      "Epoch 43/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - loss: 0.3140\n",
      "Epoch 44/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - loss: 0.2992\n",
      "Epoch 45/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - loss: 0.2845\n",
      "Epoch 46/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 0.2698\n",
      "Epoch 47/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - loss: 0.2552\n",
      "Epoch 48/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - loss: 0.2408\n",
      "Epoch 49/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - loss: 0.2267\n",
      "Epoch 50/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - loss: 0.2128\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 442ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 360ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step\n",
      "[1, 2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "\n",
    "# Example data\n",
    "X = [\n",
    "    [1, 2, 3],  # Example input sequence\n",
    "    [4, 5, 6],\n",
    "    # Add more sequences as needed\n",
    "]\n",
    "\n",
    "y = [\n",
    "    [0, 1, 0],  # Example output sequence (one-hot encoded)\n",
    "    [1, 0, 0],\n",
    "    # Add more sequences as needed\n",
    "]\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Ensure X has the correct shape (num_samples, sequence_length, feature_dimension)\n",
    "if len(X.shape) == 2:\n",
    "    X = np.expand_dims(X, axis=-1)  # Add feature dimension if missing\n",
    "\n",
    "# Assuming X is your input sequences and y is the next character in each sequence\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Train model\n",
    "model.fit(X, y, epochs=50, batch_size=32)\n",
    "\n",
    "# Generate text\n",
    "def generate_text(model, start_seq, length):\n",
    "    generated_text = start_seq.copy()\n",
    "    for _ in range(length):\n",
    "        # Ensure the input sequence has the correct shape\n",
    "        x = np.array([generated_text[-X.shape[1]:]])\n",
    "        if len(x.shape) == 2:\n",
    "            x = np.expand_dims(x, axis=-1)  # Add feature dimension if missing\n",
    "        pred = model.predict(x)\n",
    "        next_char = np.argmax(pred)\n",
    "        generated_text.append(next_char)\n",
    "    return generated_text\n",
    "\n",
    "# Example start sequence\n",
    "start_seq = [1, 2, 3]\n",
    "print(generate_text(model, start_seq, 10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d73c24f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "3. How to perform sentiment analysis using a simple CNN model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1494975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_padded: (5, 200)\n",
      "Shape of y: (5,)\n",
      "Epoch 1/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8s/step - accuracy: 1.0000 - loss: 0.6738\n",
      "Epoch 2/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy: 1.0000 - loss: 0.6423\n",
      "Epoch 3/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 1.0000 - loss: 0.6134\n",
      "Epoch 4/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step - accuracy: 1.0000 - loss: 0.5853\n",
      "Epoch 5/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 299ms/step - accuracy: 1.0000 - loss: 0.5575\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1dabaedfd00>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
    "\n",
    "# Example text data\n",
    "X = [\n",
    "    \"I love programming in Python\",\n",
    "    \"JavaScript is a versatile language\",\n",
    "    \"Machine learning is fascinating\",\n",
    "    \"I enjoy learning new technologies\",\n",
    "    \"Data science is a growing field\"\n",
    "]\n",
    "\n",
    "# Corresponding sentiment labels (binary: 0 for negative, 1 for positive)\n",
    "y = np.array([1, 1, 1, 1, 1])  # Example labels\n",
    "\n",
    "# Tokenization and padding\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X)\n",
    "sequences = tokenizer.texts_to_sequences(X)\n",
    "X_padded = pad_sequences(sequences, maxlen=200)\n",
    "\n",
    "# Check shapes\n",
    "print(f\"Shape of X_padded: {X_padded.shape}\")\n",
    "print(f\"Shape of y: {y.shape}\")\n",
    "\n",
    "# Define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(5000, 100, input_length=200))\n",
    "model.add(Conv1D(64, 3, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "model.fit(X_padded, y, epochs=5, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248e7292",
   "metadata": {},
   "source": [
    "4. How to perform Named Entity Recognition (NER) using spaCy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deee82d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load pre-trained model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Process text\n",
    "text = \"Apple is looking at buying U.K. startup for $1 billion\"\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract entities\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6048212",
   "metadata": {},
   "source": [
    "5. How to implement a simple Seq2Seq model for machine translation using LSTM in Keras?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd5b7cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 295ms/step - loss: 98.4158\n",
      "Epoch 2/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 292ms/step - loss: 102.4208\n",
      "Epoch 3/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 327ms/step - loss: 108.2404\n",
      "Epoch 4/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 510ms/step - loss: 109.9209\n",
      "Epoch 5/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 210ms/step - loss: 110.3284\n",
      "Epoch 6/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 218ms/step - loss: 110.5275\n",
      "Epoch 7/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 252ms/step - loss: 110.6587\n",
      "Epoch 8/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 304ms/step - loss: 110.6571\n",
      "Epoch 9/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 284ms/step - loss: 110.7468\n",
      "Epoch 10/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 364ms/step - loss: 110.7454\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1dab618ce50>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "\n",
    "# Define parameters\n",
    "num_encoder_tokens = 50  # Size of the encoder vocabulary\n",
    "num_decoder_tokens = 50  # Size of the decoder vocabulary\n",
    "latent_dim = 256  # Dimensionality of the LSTM layers\n",
    "batch_size = 64  # Batch size for training\n",
    "epochs = 10  # Number of epochs for training\n",
    "\n",
    "# Example data (replace with your actual data)\n",
    "encoder_input_data = np.random.random((100, 20, num_encoder_tokens))  # 100 samples, sequence length 20\n",
    "decoder_input_data = np.random.random((100, 20, num_decoder_tokens))  # 100 samples, sequence length 20\n",
    "decoder_target_data = np.random.random((100, 20, num_decoder_tokens))  # 100 samples, sequence length 20\n",
    "\n",
    "# Define encoder model\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Define decoder model\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define Seq2Seq model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "# Train model\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ed6b2a",
   "metadata": {},
   "source": [
    "6. How to generate text using a pre-trained transformer model (GPT-2)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106edf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Define function to generate text\n",
    "def generate_text(prompt, length):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    output = model.generate(input_ids, max_length=length)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Generate text\n",
    "print(generate_text('Hello, world!', 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa434da",
   "metadata": {},
   "source": [
    "7. How to apply data augmentation for text in NLP?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67026a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This exist Associate_in_Nursing illustration sentence.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Define function to replace synonyms\n",
    "def replace_synonyms(text):\n",
    "    words = text.split()\n",
    "    for i, word in enumerate(words):\n",
    "        synonyms = set()\n",
    "        for syn in wordnet.synsets(word):\n",
    "            for lemma in syn.lemmas():\n",
    "                synonyms.add(lemma.name())\n",
    "        if len(synonyms) > 1:\n",
    "            words[i] = list(synonyms)[1]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply data augmentation\n",
    "text = 'This is an example sentence.'\n",
    "augmented_text = replace_synonyms(text)\n",
    "print(augmented_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef5e0c4",
   "metadata": {},
   "source": [
    "8. How can you add an Attention Mechanism to a Seq2Seq model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27524018",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Attention\n",
    "\n",
    "\n",
    "# Define encoder model\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "\n",
    "# Define decoder model\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=[state_h, state_c])\n",
    "\n",
    "# Define attention layer\n",
    "attention_layer = Attention()\n",
    "\n",
    "# Use attention layer in decoder model\n",
    "attention_weights = attention_layer([decoder_outputs, encoder_outputs])\n",
    "\n",
    "# Define final decoder output\n",
    "decoder_outputs = Dense(num_decoder_tokens, activation='softmax')(attention_weights)\n",
    "\n",
    "# Define Seq2Seq model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea11bdcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
