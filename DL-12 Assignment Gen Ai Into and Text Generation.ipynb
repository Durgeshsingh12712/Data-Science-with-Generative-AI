{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d4ed096",
   "metadata": {},
   "source": [
    "# Assignment : Gen Ai Intro And Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1af4ca",
   "metadata": {},
   "source": [
    "Q1. What is Generative AI?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57d35ab",
   "metadata": {},
   "source": [
    "Answer : Generative AI refers to artificial intelligence technologies that can generate new content, such as text, images, videos, or music, based on the patterns and structures it has learned from training data. Unlike traditional AI, which focuses on analyzing or classifying existing data, generative AI creates novel data instances that weren't present in its training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8531e29",
   "metadata": {},
   "source": [
    "Q2. How is Generative AI different from traditional AI?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a655895",
   "metadata": {},
   "source": [
    "Answer : The primary difference lies in their objectives. Traditional AI is designed to analyze, classify, or make predictions based on existing data. In contrast, generative AI aims to generate new, synthetic data that resembles the training data but doesn't replicate it. This capability opens up a wide range of applications in content creation, data augmentation, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcdef1e",
   "metadata": {},
   "source": [
    "Q3. Name two applications of Generative AI in the industry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ef8267",
   "metadata": {},
   "source": [
    "1. Content Creation: Generative AI can produce original content, such as articles, stories, or even entire scripts, revolutionizing the media and entertainment industries.\n",
    "2. Data Augmentation: In machine learning, generative models can create synthetic data to augment training sets, improving the performance and robustness of AI models, especially in scenarios where real data is scarce or difficult to obtain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c450e41",
   "metadata": {},
   "source": [
    "Q4. What are some challenges associated with Generative AI?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e7b536",
   "metadata": {},
   "source": [
    "1. Quality and Coherence: Ensuring that the generated content is of high quality and maintains coherence can be challenging.\n",
    "2. Ethical Concerns: There are concerns regarding the misuse of generative AI for creating deepfakes, fake news, or plagiarized content.\n",
    "3. Training Requirements: Generative models require large amounts of data and computational resources to train effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ad70e5",
   "metadata": {},
   "source": [
    "Q5. Why is Generative AI important for modern applications?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea97495",
   "metadata": {},
   "source": [
    "Answer : Generative AI enables the creation of personalized content, enhances user experiences, and can automate tasks that typically require human creativity. Its ability to generate realistic synthetic data also supports advancements in various fields, from healthcare to entertainment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c79948c",
   "metadata": {},
   "source": [
    "Q6. What is probabilistic modeling in the context of Generative AI?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb33e6b",
   "metadata": {},
   "source": [
    "Answer : Probabilistic modeling involves using statistical models to predict the probability distribution of outcomes. In generative AI, this approach allows models to generate new data by sampling from the learned probability distribution of the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b05bda",
   "metadata": {},
   "source": [
    "Q7. Define a generative model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340f58c0",
   "metadata": {},
   "source": [
    "Answer : A generative model is a type of machine learning model designed to generate new data that resembles the training data. These models learn the patterns, structures, and relationships within the training data to produce novel, synthetic data instances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befd297b",
   "metadata": {},
   "source": [
    "Q8. Explain how an n-gram model works in text generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ac9989",
   "metadata": {},
   "source": [
    "Answer : An n-gram model predicts the next word in a sequence based on the context of the n-1 preceding words. For example, a bigram model (2-gram) predicts the next word based on the current word, while a trigram model (3-gram) uses the last two words for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46463ec7",
   "metadata": {},
   "source": [
    "Q9. What are the limitations of n-gram models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9c0fa0",
   "metadata": {},
   "source": [
    "Answer : \n",
    "1. Context Limitation: N-gram models can only consider a fixed-size context, limiting their ability to capture long-range dependencies.\n",
    "2. Data Sparsity: As n increases, the model requires more data to accurately estimate probabilities, which can lead to data sparsity issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35175ff",
   "metadata": {},
   "source": [
    "Q10. How can you improve the performance of an n-gram model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6899da7a",
   "metadata": {},
   "source": [
    "1. Smoothing Techniques: Applying smoothing techniques can help mitigate the issue of unseen n-grams by allocating some probability mass to unseen events.\n",
    "2. Using Higher-Order Models: While challenging due to data sparsity, using higher-order n-gram models can potentially capture more context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b46b9c",
   "metadata": {},
   "source": [
    "Q11. What is the Markov assumption, and how does it apply to text generation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c553289",
   "metadata": {},
   "source": [
    "Answer : The Markov assumption posits that the future state of a system depends only on its current state, not on any of its past states. In text generation, this means that the next word in a sequence depends only on the current or a fixed number of preceding words, simplifying the prediction process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6caabb",
   "metadata": {},
   "source": [
    "Q12. Why are probabilistic models important in generative AI?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c796a1",
   "metadata": {},
   "source": [
    "Answer : Probabilistic models allow for the generation of diverse, realistic outputs by capturing the underlying probability distribution of the training data. This capability is crucial for creating novel content that resembles real data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74405876",
   "metadata": {},
   "source": [
    "Q13. What is an autoencoder?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c03d112",
   "metadata": {},
   "source": [
    "Answer : An autoencoder is a neural network that learns to compress and reconstruct its input data. It consists of an encoder, which maps the input to a lower-dimensional representation, and a decoder, which attempts to reconstruct the original input from this representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5014271e",
   "metadata": {},
   "source": [
    "Q14. How does a VAE differ from a standard autoencoder?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ee76d8",
   "metadata": {},
   "source": [
    "Answer : A Variational Autoencoder (VAE) differs by learning a probabilistic representation of the input data. Instead of encoding the input as a fixed vector, a VAE encodes it as a probability distribution over the latent space, allowing for the generation of new data by sampling from this distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952bea20",
   "metadata": {},
   "source": [
    "Q15. Why are VAEs useful in generative modeling?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb4ab6a",
   "metadata": {},
   "source": [
    "Answer : VAEs are useful because they provide a structured, continuous, and compact representation of the data, enabling the generation of new data instances by sampling from the latent space. This capability is particularly valuable for tasks like image and text generation, where creating realistic and diverse outputs is crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c67089",
   "metadata": {},
   "source": [
    "Q16. What role does the decoder play in an autoencoder?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a35b486",
   "metadata": {},
   "source": [
    "Answer : The decoder in an autoencoder is responsible for reconstructing the original input data from the compressed representation provided by the encoder. In the context of VAEs, the decoder generates new data by sampling from the latent space and mapping it back to the original data space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6725339",
   "metadata": {},
   "source": [
    "Q17. How does the latent space affect text generation in a VAE?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d54ad1",
   "metadata": {},
   "source": [
    "Answer : The latent space in a VAE represents a lower-dimensional encoding of the input data. By sampling from this space, the VAE can generate new text that reflects the patterns and structures learned from the training data. The properties of the latent space, such as its dimensionality and the distribution of the encoded data, significantly influence the quality and diversity of the generated text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff98293a",
   "metadata": {},
   "source": [
    "Q18. What is the purpose of the Kullback-Leibler (KL) divergence term in VAEs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e65465e",
   "metadata": {},
   "source": [
    "Answer : The KL divergence term in VAEs measures the difference between the learned latent distribution and a prior distribution (usually a standard normal distribution). It acts as a regularizer, encouraging the model to learn a structured and continuous latent space. This regularization is crucial for ensuring that the generated data is realistic and diverse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e10feb0",
   "metadata": {},
   "source": [
    "Q19. How can you prevent overfitting in a VAE?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4902b3",
   "metadata": {},
   "source": [
    "Answer : Preventing overfitting in a VAE can be achieved through several methods, including:\n",
    "1. Regularization: Using the KL divergence term to regularize the latent space.\n",
    "2. Data Augmentation: Increasing the size and diversity of the training dataset.\n",
    "3. Early Stopping: Stopping the training process before the model starts to overfit.\n",
    "4. Reducing Model Complexity: Using simpler models or reducing the number of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef909cf",
   "metadata": {},
   "source": [
    "Q20. What is a transformer model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7358f3",
   "metadata": {},
   "source": [
    "Answer : A transformer model is a type of neural network architecture primarily used for natural language processing tasks. It relies on self-attention mechanisms to weigh the importance of different input elements relative to each other, allowing it to handle long-range dependencies more effectively than traditional recurrent neural networks (RNNs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a90e236",
   "metadata": {},
   "source": [
    "Q21. Explain the purpose of self-attention in transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbf1d63",
   "metadata": {},
   "source": [
    "Answer : Self-attention in transformers allows the model to attend to all positions in the input sequence simultaneously and weigh their importance. This mechanism enables the model to capture long-range dependencies and contextual relationships more effectively, making it particularly useful for tasks like language translation and text generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4669240",
   "metadata": {},
   "source": [
    "Q22. How does a GPT model generate text?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94778585",
   "metadata": {},
   "source": [
    "Answer : A GPT (Generative Pre-trained Transformer) model generates text by predicting the next word in a sequence based on the context provided by the preceding words. It uses a transformer architecture and is pre-trained on a large corpus of text data, allowing it to learn patterns and structures of language. During generation, the model samples from its output probabilities to create coherent and contextually relevant text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dbab28",
   "metadata": {},
   "source": [
    "Q23. Explain why VAEs are commonly used for unsupervised learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2917ae2f",
   "metadata": {},
   "source": [
    "Answer : VAEs are commonly used for unsupervised learning tasks because they can learn to represent data in a lower-dimensional latent space without requiring labeled data. This capability makes them particularly useful for tasks like dimensionality reduction, anomaly detection, and generative modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84262255",
   "metadata": {},
   "source": [
    "Q24. What are the key differences between a GPT model and an RNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463f495e",
   "metadata": {},
   "source": [
    "Answer : The key differences include:\n",
    "1. Architecture: GPT models use a transformer architecture, while RNNs use recurrent connections.\n",
    "2. Handling Long-Range Dependencies: GPT models handle long-range dependencies more effectively due to their self-attention mechanism.\n",
    "3. Training: GPT models are typically pre-trained on large datasets and fine-tuned for specific tasks, whereas RNNs are often trained from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6b58fb",
   "metadata": {},
   "source": [
    "Q25. How does fine-tuning improve a pre-trained GPT model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e650a35",
   "metadata": {},
   "source": [
    "Answer : Fine-tuning a pre-trained GPT model involves adjusting its weights to fit a specific task or dataset. This process improves the model's performance on the target task by allowing it to learn task-specific patterns and nuances, making it more effective for applications like text classification, sentiment analysis, and text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55a99c8",
   "metadata": {},
   "source": [
    "Q26. What is zero-shot learning in the context of GPT models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402a9a9c",
   "metadata": {},
   "source": [
    "Answer : Zero-shot learning refers to the ability of a model to perform a task without having been explicitly trained on it. In the context of GPT models, zero-shot learning enables the generation of text or answers to questions on topics the model has not seen during training, leveraging its understanding of language patterns and context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e519be",
   "metadata": {},
   "source": [
    "Q27. Describe how prompt engineering can impact GPT model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7468dfd7",
   "metadata": {},
   "source": [
    "Answer : Prompt engineering involves crafting specific input prompts to elicit desired responses from a GPT model. By carefully designing these prompts, users can significantly improve the model's performance, guiding it to generate more accurate, relevant, and contextually appropriate text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9916f71f",
   "metadata": {},
   "source": [
    "Q28. Why are large datasets essential for training GPT models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576aac23",
   "metadata": {},
   "source": [
    "Answer : Large datasets are crucial for training GPT models because they provide the model with a vast amount of text data to learn from. This enables the model to capture a wide range of language patterns, nuances, and contexts, which is essential for generating coherent and contextually relevant text. The more data the model is trained on, the better it can generalize and produce high-quality text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9d3981",
   "metadata": {},
   "source": [
    "Q29. What are potential ethical concerns with GPT models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731c2698",
   "metadata": {},
   "source": [
    "Answer : Potential ethical concerns with GPT models include:\n",
    "1. Misinformation and Disinformation: GPT models can generate convincing but false information, contributing to the spread of misinformation.\n",
    "2. Bias and Stereotyping: Models can perpetuate biases present in the training data, leading to stereotyping or discriminatory content.\n",
    "3. Privacy: GPT models might generate text that inadvertently reveals sensitive information from the training data.\n",
    "4. Misuse: The ability to generate realistic text can be misused for malicious purposes, such as creating deepfakes or spam content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cc62be",
   "metadata": {},
   "source": [
    "Q30. How does the attention mechanism contribute to GPT’s ability to handle long-range dependencies?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f31f7ca",
   "metadata": {},
   "source": [
    "Answer : The attention mechanism in GPT models allows the model to focus on different parts of the input sequence simultaneously, rather than processing them sequentially. This enables the model to capture long-range dependencies more effectively by directly attending to relevant words or phrases, regardless of their position in the sequence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34916c3a",
   "metadata": {},
   "source": [
    "Q31. What are some limitations of GPT models for real-world applications?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cccbc3",
   "metadata": {},
   "source": [
    "Answer : Some limitations include:\n",
    "1. Contextual Understanding: While GPT models can generate coherent text, their understanding of context can be limited, leading to inappropriate or irrelevant responses in certain situations.\n",
    "2. Common Sense and World Knowledge: GPT models lack real-world experience and common sense, which can result in generating text that, while grammatically correct, is nonsensical or impractical.\n",
    "3. Bias and Fairness: GPT models can reflect biases present in the training data, which can be a significant issue for applications requiring fairness and neutrality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb2b63c",
   "metadata": {},
   "source": [
    "Q32. How can GPT models be adapted for domain-specific text generation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f0d597",
   "metadata": {},
   "source": [
    "Answer : GPT models can be adapted for domain-specific text generation through:\n",
    "1. Fine-Tuning: Training the model on a domain-specific dataset to adjust its weights and improve performance in that domain.\n",
    "2. Prompt Engineering: Crafting specific prompts that guide the model to generate text relevant to the desired domain.\n",
    "3. Domain-Specific Pre-Training: Pre-training the model on a large corpus of text from the specific domain before fine-tuning it for a particular task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c66b8c",
   "metadata": {},
   "source": [
    "Q33. What are some common metrics for evaluating text generation quality?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b763615",
   "metadata": {},
   "source": [
    "Answer : Common metrics include:\n",
    "1. Perplexity: Measures how well the model predicts the next word in a sequence, with lower perplexity indicating better performance.\n",
    "2. BLEU Score: Evaluates the similarity between generated text and reference text, with higher scores indicating greater similarity.\n",
    "3. ROUGE Score: Measures the overlap between generated text and reference text, focusing on n-grams and word sequences.\n",
    "4. Human Evaluation: Involves human evaluators assessing the quality, coherence, and relevance of generated text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e1cdef",
   "metadata": {},
   "source": [
    "Q24. Explain the difference between deterministic and probabilistic text generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82eb807",
   "metadata": {},
   "source": [
    "Answer : Deterministic text generation involves generating text based on fixed rules or mappings, where the output is entirely predictable given the input. Probabilistic text generation, on the other hand, involves generating text based on probability distributions, allowing for variability and uncertainty in the output. GPT models use probabilistic text generation, sampling from predicted probability distributions to create diverse and contextually relevant text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fb2572",
   "metadata": {},
   "source": [
    "Q35. How does beam search improve text generation in language models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7ee5b0",
   "metadata": {},
   "source": [
    "Answer : Beam search is a decoding strategy used in language models to generate text. It improves text generation by:\n",
    "1. Exploring Multiple Hypotheses: Beam search considers multiple possible sequences of words simultaneously, rather than choosing the most likely word at each step.\n",
    "2. Optimizing for Coherence: By evaluating multiple hypotheses, beam search can generate more coherent and contextually relevant text.\n",
    "3. Balancing Exploration and Exploitation: Beam search balances the need to explore different possibilities with the need to focus on the most promising sequences, leading to higher-quality generated text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68152ab4",
   "metadata": {},
   "source": [
    "# Practical:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0869d516",
   "metadata": {},
   "source": [
    "1. Generate a random sentence using probabilistic modeling (Markov Chain)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29aa47c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cat is on the mat\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Example sentence\n",
    "sentence = \"The cat is on the mat\"\n",
    "\n",
    "# Split the sentence into words\n",
    "words = sentence.split()\n",
    "\n",
    "# Create a Markov Chain transition matrix\n",
    "transition_matrix = {}\n",
    "for i in range(len(words) - 1):\n",
    "    if words[i] not in transition_matrix:\n",
    "        transition_matrix[words[i]] = []\n",
    "    transition_matrix[words[i]].append(words[i + 1])\n",
    "\n",
    "# Function to generate a random sentence\n",
    "def generate_sentence(start_word, max_length):\n",
    "    sentence = [start_word]\n",
    "    for _ in range(max_length):\n",
    "        if sentence[-1] not in transition_matrix:\n",
    "            break\n",
    "        next_word = random.choice(transition_matrix[sentence[-1]])\n",
    "        sentence.append(next_word)\n",
    "    return ' '.join(sentence)\n",
    "\n",
    "# Generate a random sentence\n",
    "print(generate_sentence(\"The\", 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a58abfa",
   "metadata": {},
   "source": [
    "2. Build a simple Autoencoder model using Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaf36f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1us/step\n",
      "Epoch 1/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 12ms/step - loss: -421718464.0000 - val_loss: -3864686336.0000\n",
      "Epoch 2/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: -6407400448.0000 - val_loss: -14518860800.0000\n",
      "Epoch 3/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: -19136772096.0000 - val_loss: -30522419200.0000\n",
      "Epoch 4/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - loss: -36995534848.0000 - val_loss: -51000832000.0000\n",
      "Epoch 5/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - loss: -58952790016.0000 - val_loss: -75510349824.0000\n",
      "Epoch 6/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: -86377414656.0000 - val_loss: -103699947520.0000\n",
      "Epoch 7/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - loss: -117195841536.0000 - val_loss: -135387635712.0000\n",
      "Epoch 8/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - loss: -149610807296.0000 - val_loss: -170623614976.0000\n",
      "Epoch 9/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: -188626894848.0000 - val_loss: -209191534592.0000\n",
      "Epoch 10/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: -230768197632.0000 - val_loss: -251053506560.0000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid dtype: str672",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe cat is on the mat\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     27\u001b[0m sentence_vector \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([sentence])\n\u001b[1;32m---> 28\u001b[0m encoded_sentence \u001b[38;5;241m=\u001b[39m \u001b[43mautoencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence_vector\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(encoded_sentence)\n",
      "File \u001b[1;32mc:\\Users\\HEART\\anaconda3\\envs\\NLP\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\HEART\\anaconda3\\envs\\NLP\\lib\\site-packages\\optree\\ops.py:766\u001b[0m, in \u001b[0;36mtree_map\u001b[1;34m(func, tree, is_leaf, none_is_leaf, namespace, *rests)\u001b[0m\n\u001b[0;32m    764\u001b[0m leaves, treespec \u001b[38;5;241m=\u001b[39m _C\u001b[38;5;241m.\u001b[39mflatten(tree, is_leaf, none_is_leaf, namespace)\n\u001b[0;32m    765\u001b[0m flat_args \u001b[38;5;241m=\u001b[39m [leaves] \u001b[38;5;241m+\u001b[39m [treespec\u001b[38;5;241m.\u001b[39mflatten_up_to(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rests]\n\u001b[1;32m--> 766\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtreespec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflat_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid dtype: str672"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "# Load the IMDB dataset\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=10000)\n",
    "\n",
    "# Preprocess the data\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=100)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=100)\n",
    "\n",
    "# Define the autoencoder model\n",
    "input_layer = Input(shape=(100,))\n",
    "encoded = Dense(64, activation='relu')(input_layer)\n",
    "decoded = Dense(100, activation='sigmoid')(encoded)\n",
    "\n",
    "# Compile the model\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "# Train the model\n",
    "autoencoder.fit(x_train, x_train, epochs=10, batch_size=32, validation_data=(x_test, x_test))\n",
    "\n",
    "# Use the model to learn a compressed representation of a sentence\n",
    "sentence = \"The cat is on the mat\"\n",
    "sentence_vector = np.array([sentence])\n",
    "encoded_sentence = autoencoder.predict(sentence_vector)\n",
    "print(encoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ec5af7",
   "metadata": {},
   "source": [
    "3. Fine-tune a pre-trained GPT-2 model using Hugging Face transformers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43ec7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "# Load the pre-trained GPT-2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Define a custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, text_data):\n",
    "        self.text_data = text_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.text_data[idx]\n",
    "        inputs = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=512,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten()\n",
    "        }\n",
    "\n",
    "# Load the custom text data\n",
    "text_data = [\"The cat is on the mat\", \"The dog is in the house\"]\n",
    "\n",
    "# Create a dataset and data loader\n",
    "dataset = CustomDataset(text_data)\n",
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Fine-tune the model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in data_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = input_ids.clone()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "    print(f'Epoch {epoch+1}, Loss: {total_loss / len(data_loader)}')\n",
    "\n",
    "# Generate text\n",
    "input_ids = tokenizer.encode(\"The cat\", return_tensors='pt').to(device)\n",
    "output = model.generate(input_ids, max_length=50)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30aee19d",
   "metadata": {},
   "source": [
    "4. Implement a text generation model using a simple Recurrent Neural Network (RNN) in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2568c70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, SimpleRNN\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "\n",
    "# Custom data\n",
    "sentences = [\"The cat is on the mat\", \"The dog is in the house\"]\n",
    "words = set(' '.join(sentences).split())\n",
    "word_to_index = {word: i for i, word in enumerate(words)}\n",
    "index_to_word = {i: word for word, i in word_to_index.items()}\n",
    "\n",
    "# Preprocess the data\n",
    "X = []\n",
    "y = []\n",
    "for sentence in sentences:\n",
    "    words_in_sentence = sentence.split()\n",
    "    for i in range(len(words_in_sentence) - 1):\n",
    "        X.append(word_to_index[words_in_sentence[i]])\n",
    "        y.append(word_to_index[words_in_sentence[i + 1]])\n",
    "\n",
    "X = np.array(X)\n",
    "y = to_categorical(y)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(64, input_shape=(1,)))\n",
    "model.add(Dense(len(words), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Reshape the data\n",
    "X = X.reshape((X.shape[0], 1))\n",
    "\n",
    "# Train the model\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=5, min_delta=0.001)\n",
    "model.fit(X, y, epochs=100, batch_size=32, callbacks=[early_stopping])\n",
    "\n",
    "# Generate text\n",
    "def generate_text(start_word, max_length):\n",
    "    sentence = [start_word]\n",
    "    for _ in range(max_length):\n",
    "        input_word = np.array([word_to_index[sentence[-1]]]).reshape((1, 1))\n",
    "        output = model.predict(input_word)\n",
    "        next_word_index = np.argmax(output)\n",
    "        sentence.append(index_to_word[next_word_index])\n",
    "    return ' '.join(sentence)\n",
    "\n",
    "print(generate_text(\"The\", 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adad873",
   "metadata": {},
   "source": [
    "5. Generate a sequence of text using an LSTM-based model in TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31670cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "import numpy as np\n",
    "\n",
    "# Custom data\n",
    "sentences = [\"The cat is on the mat\", \"The dog is in the house\"]\n",
    "words = set(' '.join(sentences).split())\n",
    "word_to_index = {word: i for i, word in enumerate(words)}\n",
    "index_to_word = {i: word for word, i in word_to_index.items()}\n",
    "\n",
    "# Preprocess the data\n",
    "X = []\n",
    "y = []\n",
    "for sentence in sentences:\n",
    "    words_in_sentence = sentence.split()\n",
    "    for i in range(len(words_in_sentence) - 1):\n",
    "        X.append(word_to_index[words_in_sentence[i]])\n",
    "        y.append(word_to_index[words_in_sentence[i + 1]])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(1,)))\n",
    "model.add(Dense(len(words), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Reshape the data\n",
    "X = X.reshape((X.shape[0], 1))\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs=100, batch_size=32)\n",
    "\n",
    "# Generate text\n",
    "def generate_text(start_word, max_length):\n",
    "    sentence = [start_word]\n",
    "    for _ in range(max_length):\n",
    "        input_word = np.array([word_to_index[sentence[-1]]]).reshape((1, 1))\n",
    "        output = model.predict(input_word)\n",
    "        next_word_index = np.argmax(output)\n",
    "        sentence.append(index_to_word[next_word_index])\n",
    "    return ' '.join(sentence)\n",
    "\n",
    "print(generate_text(\"The\", 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae25548",
   "metadata": {},
   "source": [
    "6. Build a program that uses GPT-2 from Hugging Face to generate a story based on a custom prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caaafe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "# Load the pre-trained GPT-2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Define a custom prompt\n",
    "prompt = \"Once upon a time in a land far, far away\"\n",
    "\n",
    "# Generate text\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "output = model.generate(input_ids, max_length=100)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144e9b85",
   "metadata": {},
   "source": [
    "7. Implement a simple text generation model using a GRU-based architecture in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2af2867",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, GRU\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "\n",
    "# Custom data\n",
    "sentences = [\"The cat is on the mat\", \"The dog is in the house\"]\n",
    "words = set(' '.join(sentences).split())\n",
    "word_to_index = {word: i for i, word in enumerate(words)}\n",
    "index_to_word = {i: word for word, i in word_to_index.items()}\n",
    "\n",
    "# Preprocess the data\n",
    "X = []\n",
    "y = []\n",
    "for sentence in sentences:\n",
    "    words_in_sentence = sentence.split()\n",
    "    for i in range(len(words_in_sentence) - 1):\n",
    "        X.append(word_to_index[words_in_sentence[i]])\n",
    "        y.append(word_to_index[words_in_sentence[i + 1]])\n",
    "\n",
    "X = np.array(X)\n",
    "y = to_categorical(y)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(GRU(64, input_shape=(1,)))\n",
    "model.add(Dense(len(words), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Reshape the data\n",
    "X = X.reshape((X.shape[0], 1))\n",
    "\n",
    "# Train the model\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=5, min_delta=0.001)\n",
    "model.fit(X, y, epochs=100, batch_size=32, callbacks=[early_stopping])\n",
    "\n",
    "# Generate text\n",
    "def generate_text(start_word, max_length):\n",
    "    sentence = [start_word]\n",
    "    for _ in range(max_length):\n",
    "        input_word = np.array([word_to_index[sentence[-1]]]).reshape((1, 1))\n",
    "        output = model.predict(input_word)\n",
    "        next_word_index = np.argmax(output)\n",
    "        sentence.append(index_to_word[next_word_index])\n",
    "    return ' '.join(sentence)\n",
    "\n",
    "print(generate_text(\"The\", 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8463b352",
   "metadata": {},
   "source": [
    "8. Implement a simple LSTM-based text generation model from scratch using Keras and train it on a custom data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5d72d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "\n",
    "# Custom data\n",
    "sentences = [\"The cat is on the mat\", \"The dog is in the house\"]\n",
    "words = set(' '.join(sentences).split())\n",
    "word_to_index = {word: i for i, word in enumerate(words)}\n",
    "index_to_word = {i: word for word, i in word_to_index.items()}\n",
    "\n",
    "# Preprocess the data\n",
    "X = []\n",
    "y = []\n",
    "for sentence in sentences:\n",
    "    words_in_sentence = sentence.split()\n",
    "    for i in range(len(words_in_sentence) - 1):\n",
    "        X.append(word_to_index[words_in_sentence[i]])\n",
    "        y.append(word_to_index[words_in_sentence[i + 1]])\n",
    "\n",
    "X = np.array(X)\n",
    "y = to_categorical(y)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(1,)))\n",
    "model.add(Dense(len(words), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Reshape the data\n",
    "X = X.reshape((X.shape[0], 1))\n",
    "\n",
    "# Train the model\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=5, min_delta=0.001)\n",
    "model.fit(X, y, epochs=100, batch_size=32, callbacks=[early_stopping])\n",
    "\n",
    "# Generate text\n",
    "def generate_text(start_word, max_length):\n",
    "    sentence = [start_word]\n",
    "    for _ in range(max_length):\n",
    "        input_word = np.array([word_to_index[sentence[-1]]]).reshape((1, 1))\n",
    "        output = model.predict(input_word)\n",
    "        next_word_index = np.argmax(output)\n",
    "        sentence.append(index_to_word[next_word_index])\n",
    "    return ' '.join(sentence)\n",
    "\n",
    "print(generate_text(\"The\", 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65739a6b",
   "metadata": {},
   "source": [
    "9. Create a script to implement GPT-2-based text generation with beam search decoding to generate text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dcbe0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "# Load the pre-trained GPT-2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Define a custom prompt\n",
    "prompt = \"Once upon a time in a land far, far away\"\n",
    "\n",
    "# Generate text with beam search decoding\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "output = model.generate(input_ids, max_length=100, num_beams=5, no_repeat_ngram_size=2)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85286247",
   "metadata": {},
   "source": [
    "10. Implement a text generation script using GPT-2 with a custom temperature setting for diversity in output text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2d7504",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "# Load the pre-trained GPT-2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Define a custom prompt\n",
    "prompt = \"Once upon a time in a land far, far away\"\n",
    "\n",
    "# Generate text with custom temperature setting\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "output = model.generate(input_ids, max_length=100, temperature=1.5)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589d21be",
   "metadata": {},
   "source": [
    "11. Create a script to implement temperature sampling with GPT-2, experimenting with different values to generate creative text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5da2508",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "# Load the pre-trained GPT-2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Define a custom prompt\n",
    "prompt = \"Once upon a time in a land far, far away\"\n",
    "\n",
    "# Experiment with different temperature values\n",
    "temperatures = [0.5, 1.0, 1.5, 2.0]\n",
    "for temperature in temperatures:\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    output = model.generate(input_ids, max_length=100, temperature=temperature)\n",
    "    print(f\"Temperature: {temperature}\")\n",
    "    print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf465ae",
   "metadata": {},
   "source": [
    "12. Implement text generation using a simple custom attention-based architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee61753",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, LSTM, Attention, Input\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "\n",
    "# Custom data\n",
    "sentences = [\"The cat is on the mat\", \"The dog is in the house\"]\n",
    "words = set(' '.join(sentences).split())\n",
    "word_to_index = {word: i for i, word in enumerate(words)}\n",
    "index_to_word = {i: word for word, i in word_to_index.items()}\n",
    "\n",
    "# Preprocess the data\n",
    "X = []\n",
    "y = []\n",
    "for sentence in sentences:\n",
    "    words_in_sentence = sentence.split()\n",
    "    for i in range(len(words_in_sentence) - 1):\n",
    "        X.append(word_to_index[words_in_sentence[i]])\n",
    "        y.append(word_to_index[words_in_sentence[i + 1]])\n",
    "\n",
    "X = np.array(X)\n",
    "y = to_categorical(y)\n",
    "\n",
    "# Define the model\n",
    "input_layer = Input(shape=(1,))\n",
    "embedding_layer = Dense(64, activation='relu')(input_layer)\n",
    "lstm_layer = LSTM(64, return_sequences=True)(embedding_layer)\n",
    "attention_layer = Attention()([lstm_layer, lstm_layer])\n",
    "output_layer = Dense(len(words), activation='softmax')(attention_layer)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Reshape the data\n",
    "X = X.reshape((X.shape[0], 1))\n",
    "\n",
    "# Train the model\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=5, min_delta=0.001)\n",
    "model.fit(X, y, epochs=100, batch_size=32, callbacks=[early_stopping])\n",
    "\n",
    "# Generate text\n",
    "def generate_text(start_word, max_length):\n",
    "    sentence = [start_word]\n",
    "    for _ in range(max_length):\n",
    "        input_word = np.array([word_to_index[sentence[-1]]]).reshape((1, 1))\n",
    "        output = model.predict(input_word)\n",
    "        next_word_index = np.argmax(output)\n",
    "        sentence.append(index_to_word[next_word_index])\n",
    "    return ' '.join(sentence)\n",
    "\n",
    "print(generate_text(\"The\", 10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
