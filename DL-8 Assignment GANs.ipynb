{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment : GANs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. What does GAN stand for, and what is its main purpose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer : GAN stands for Generative Adversarial Network. Its main purpose is to generate synthetic data (e.g., images, text) that closely resembles real-world data by adversarial training between two networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. Explain the concept of the \"discriminator\" in GANs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer : The discriminator is a neural network trained to distinguish between real data (e.g., real images) and fake data generated by the generator. It outputs a probability score indicating whether an input is real or fake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. How does a GAN work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer : \n",
    "1. The generator creates synthetic data from random noise.\n",
    "2. The discriminator evaluates the generator’s output.\n",
    "3. The generator learns to fool the discriminator, while the discriminator improves its ability to detect fakes. This adversarial process refines both networks until the generator produces realistic data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. What is the generator's role in a GAN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer : The generator maps random noise (input) to synthetic data (e.g., images). It learns to mimic the real data distribution by minimizing the discriminator’s ability to detect its outputs as fake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. What is the loss function used in the training of GANs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer : \n",
    "1. Traditional GANs: Use a minimax loss (binary cross-entropy) to optimize the generator and discriminator.\n",
    "2. WGAN: Uses the Wasserstein loss (Earth-Mover distance) to stabilize training.\n",
    "3. Modern variants: May use hinge loss, least squares, or other custom losses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. What is the difference between a WGAN and a traditional GAN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answe : WGAN:\n",
    "1. Uses the Wasserstein loss (Earth-Mover distance).\n",
    "2. Enforces Lipschitz continuity via gradient clipping or a gradient penalty.\n",
    "3. Avoids mode collapse and provides more stable training.\n",
    "Traditional GAN:\n",
    "1. Uses binary cross-entropy loss.\n",
    "2. Prone to instability (e.g., vanishing gradients, mode collapse)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. How does the training of the generator differ from that of the discriminator?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer :\n",
    "Generator:\n",
    "1. Trained to minimize the discriminator’s ability to detect fakes.\n",
    "2. Updates its weights to generate more realistic data.\n",
    "\n",
    "Discriminator:\n",
    "1. Trained to maximize its ability to distinguish real vs. fake data.\n",
    "2. Updates its weights to improve classification accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. What is a DCGAN, and how is it different from a traditional GAN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer : DCGAN (Deep Convolutional GAN) uses convolutional neural networks (CNNs) for image generation.\n",
    "\n",
    "Key differences from traditional GANs:\n",
    "1. Uses strided convolutions (generator) and fractions (discriminator) instead of pooling layers.\n",
    "2. Removes fully connected layers for better scalability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q9. Explain the concept of \"controllable generation\" in the context of GANs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer : Controllable generation allows users to manipulate specific attributes (e.g., style, pose, or color) in generated outputs by conditioning the generator on input parameters or latent variables (e.g., StyleGAN’s style codes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q10. What is the primary goal of training a GAN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer : The goal is to reach an equilibrium where:\n",
    "\n",
    "1. The generator produces data so realistic that the discriminator cannot reliably distinguish it from real data.\n",
    "2. The discriminator achieves ~50% accuracy (indicating it can’t reliably tell real/fake apart)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q11. What are the limitations of GANs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer : \n",
    "1. Training instability: Requires careful hyperparameter tuning.\n",
    "2. Mode collapse: Generator produces limited variations.\n",
    "3. Evaluation difficulty: No clear metrics for quality/diversity.\n",
    "4. Computational cost: Requires large datasets and GPU resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q12.  What are StyleGANs, and what makes them unique?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer : \n",
    "1. StyleGAN separates content (shape/structure) and style (appearance) in generated images.\n",
    "2. Unique features:\n",
    "      1. Adaptive instance normalization (AdaIN) for fine-grained style control.\n",
    "      2. Progressive growing (training at increasing resolutions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q13. What is the role of noise in a GANs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer : Noise (e.g., Gaussian noise) is the generator’s input. It introduces randomness, enabling diverse outputs and preventing the generator from collapsing to a single output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q14. Describe the architecture of a typical GANs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer : A typical Generative Adversarial Network (GAN) consists of two neural networks: a generator and a discriminator. The architecture of a GAN can be described as follows:\n",
    "\n",
    "1. Generator Network: The generator network takes a random noise vector as input and generates a synthetic data sample that aims to mimic the real data distribution. The generator network typically consists of a series of transposed convolutional layers (also known as upsampling layers) followed by activation functions such as ReLU or tanh.\n",
    "\n",
    "2. Discriminator Network: The discriminator network takes a data sample (either real or synthetic) as input and outputs a probability that the sample is real. The discriminator network typically consists of a series of convolutional layers followed by activation functions such as Leaky ReLU.\n",
    "\n",
    "3. Loss Functions: The generator and discriminator networks are trained simultaneously using two loss functions:\n",
    "\n",
    "          1. Generator Loss: The generator loss is typically measured using the binary cross-entropy loss function, which encourages the generator to produce synthetic samples that are indistinguishable from real samples.\n",
    "          2. Discriminator Loss: The discriminator loss is also measured using the binary cross-entropy loss function, which encourages the discriminator to correctly distinguish between real and synthetic samples.\n",
    "4. Training: The GAN is trained using an alternating optimization scheme, where the discriminator is trained for one iteration, followed by the generator being trained for one iteration. This process is repeated for many iterations, with the generator and discriminator networks being updated simultaneously.\n",
    "5. Architecture Variants: There are many variants of the basic GAN architecture, including:\n",
    "\n",
    "            1. Deep Convolutional GANs (DCGANs): Use convolutional layers in both the generator and discriminator networks.\n",
    "            2. Conditional GANs (CGANs): Use a conditional input to the generator and discriminator networks to generate samples conditioned on a specific label or attribute.\n",
    "            3. Wasserstein GANs (WGANs): Use a different loss function, such as the Earth Mover's distance, to train the generator and discriminator networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q15. How does the loss function in a WGAN improve training stability?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer : \n",
    "1. The Wasserstein loss provides smoother gradients, avoiding the vanishing gradient problem of traditional GANs.\n",
    "2. The gradient penalty enforces Lipschitz continuity, ensuring stable training dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q16. What challenges do GANs face during training, and how can they be addressed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer : \n",
    "1. Challenges: Mode collapse, vanishing gradients, instability.\n",
    "2. Solutions:\n",
    "      1. Use WGAN or LSGAN loss.\n",
    "      2. Add regularization (e.g., gradient penalty, spectral normalization).\n",
    "      3. Use techniques like minibatch discrimination or diversity-sensitive loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q17. How does DCGAN help improve image generation in GANs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer : \n",
    "1. Uses CNNs to exploit spatial hierarchies in images.\n",
    "2. Architectural improvements (e.g., no max-pooling, batch normalization) lead to sharper, higher-quality images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q18. What are the key differences between a traditional GAN and a StyleGAN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer : \n",
    "1. DCGAN: Focuses on stable image generation via CNN architecture.\n",
    "2. StyleGAN: Focuses on style control (e.g., separating style and content, progressive growing for high-resolution outputs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q19. How does the discriminator decide whether an image is real or fake in a GAN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer : The discriminator learns discriminative features (e.g., edges, textures) through training on real/fake data. It outputs a score (e.g., 0 for fake, 1 for real) based on learned patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q20. What is the main advantage of using GANs in image generation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer : GANs generate highly realistic and diverse outputs, making them ideal for creative applications (e.g., art, video games) and data augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q21. How can GANs be used in real-world applications?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer :\n",
    "1. Image synthesis (e.g., faces, art).\n",
    "2. Data augmentation for small datasets.\n",
    "3. Style transfer (e.g., CycleGAN for domain transfer).\n",
    "4. Medical imaging (e.g., generating synthetic patient data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q22. What is Mode Collapse in GANs, and how can it be prevented?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer :\n",
    "1. Mode Collapse: The generator produces limited variations (e.g., only one type of face).\n",
    "2. Prevention:\n",
    "      1. Use WGAN or LSGAN loss.\n",
    "      2. Add diversity-promoting losses (e.g., feature matching).\n",
    "      3. Regularization (e.g., gradient penalty, minibatch discrimination)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Practical Google Colab Link : https://colab.research.google.com/drive/1iUCsU4pr1GZN4zGNDPDiIwJK8wm4QsiD?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
