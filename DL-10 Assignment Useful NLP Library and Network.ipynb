{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful NLP Library And Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. What is NLTK?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer : Natural Language Toolkit (NLTK) is a Python library for NLP tasks like tokenization, POS tagging, parsing, and semantic reasoning. It provides pre-trained models and corpora for text analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. What is SpaCy and how does it differ from NLTK?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer : \n",
    "SpaCy: A modern Python library optimized for production NLP, focusing on speed and accuracy.\n",
    "\n",
    "Differences:\n",
    "    \n",
    "1. SpaCy is faster and better for real-world applications.\n",
    "2. Offers better dependency parsing and NER out-of-the-box.\n",
    "3. More streamlined API compared to NLTK’s broader research focus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. What is the purpose of TextBlob in NLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer : TextBlob simplifies common NLP tasks (sentiment analysis, POS tagging, translation) with a simple API, wrapping NLTK and Pattern libraries for ease of use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. What is Stanford NLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer : Stanford NLP is a Java-based toolkit for advanced NLP tasks like dependency parsing, NER, and sentiment analysis, known for high accuracy in syntactic and semantic analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. Explain what Recurrent Neural Networks (RNN) are?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer : RNNs process sequential data by maintaining a hidden state that captures context from previous inputs. They’re used for tasks like language modeling and machine translation but struggle with long-term dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. What is the main advantage of using LSTM over RNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer : LSTMs (Long Short-Term Memory) use gates (input, forget, output) to selectively remember or forget information, addressing the vanishing gradient problem and enabling handling of long sequences better than standard RNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. What are Bi-directional LSTMs, and how do they differ from standard LSTMs ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer :Bi-LSTMs process data in both forward and backward directions, combining context from past and future inputs. This improves performance in tasks like POS tagging and NER."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. What is the purpose of a Stacked LSTM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer : Stacked LSTMs use multiple LSTM layers to learn hierarchical features (e.g., lower layers detect simple patterns, higher layers capture complex semantics), improving model depth and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q9. What is the significance of RNNs in NLP tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer : Recurrent Neural Networks (RNNs) are highly significant in Natural Language Processing (NLP) tasks due to their ability to model sequential data, which is a fundamental characteristic of language. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q10. How does a GRU (Gated Recurrent Unit) differ from an LSTM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer : GRU (Gated Recurrent Unit) simplifies LSTM’s gates:\n",
    "Uses reset and update gates instead of three gates.\n",
    "Faster and requires fewer parameters but may sacrifice some expressive power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q11. What are the key features of NLTK's tokenization process?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer : Splits text into tokens (words, sentences).\n",
    "Handles contractions (e.g., \"don’t\" → [\"don\", \"’t\"]).\n",
    "Uses pre-trained models (e.g., word_tokenize)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q12. How do you perform named entity recognition (NER) using SpaCy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "# doc = nlp(\"Elon Musk founded SpaceX in 2002.\")\n",
    "# for ent in doc.ents:\n",
    "#   print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q13. What is Word2Vec and how does it represent words\n",
    "Explain the difference between Bag of Words (BoW) and Word2Vec?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer : Word2Vec generates dense word vectors (embeddings) by predicting context words (Skip-Gram) or predicting a word from its context (CBOW). Vectors capture semantic relationships (e.g., \"king\" - \"man\" + \"woman\" ≈ \"queen\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q14. How does TextBlob handle sentiment analysis?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer : TextBlob is a popular Python library that provides simple tools for processing textual data, including sentiment analysis. It’s built on top of NLTK and Pattern and makes it easy to perform NLP tasks with minimal code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q15. How do you train a custom NER model using SpaCy\n",
    "What is the role of the attention mechanism in LSTMs and GRUs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer : Attention mechanisms allow models to focus on relevant parts of the input sequence when generating outputs (e.g., in translation), improving performance on long sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q16. What is the difference between tokenization and lemmatization in NLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer : \n",
    "Tokenization: Split text into tokens (words, sentences).\n",
    "\n",
    "Lemmatization: Reduces words to their base form (lemma), e.g., \"running\" → \"run\", requiring POS tags (common in SpaCy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q17. How do you perform text normalization in NLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer : Lowercasing, removing punctuation, expanding contractions, removing special characters, and standardizing text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q18. What is the purpose of frequency distribution in NLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer : Measures how often words appear in a corpus, useful for identifying common terms, removing stopwords, or analyzing text patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q19. What are co-occurrence vectors in NLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer : In Natural Language Processing (NLP), co-occurrence vectors are a representation of words based on the context in which they appear together within a corpus (a large collection of text). These vectors capture the relationship between words by observing how frequently they co-occur within a defined context, usually a sliding window of words or within the same document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q20. How is Word2Vec used to find the relationship between words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer : Vectors representing words based on their neighboring words (e.g., in Word2Vec), capturing semantic meaning through context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q21. How does a Bi-LSTM improve NLP tasks compared to a regular LSTM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer : Bi-LSTMs process data in both directions, capturing context from past and future inputs, improving performance in tasks like POS tagging and NER."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q22. What is the difference between a GRU and an LSTM in terms of gate structures?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer : \n",
    "LSTM: Input, forget, and output gates.\n",
    "\n",
    "GRU: Reset and update gates (combined functionality, simpler but potentially less powerful)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q23. How does Stanford NLP’s dependency parsing work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer : Analyzes grammatical structure by identifying relationships between words (e.g., subject-verb dependencies), useful for syntax analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q24. How does tokenization affect downstream NLP tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer : Poor tokenization (e.g., splitting \"don’t\" into \"don\" and \"’t\") can degrade downstream tasks like NER or sentiment analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q25. What are some common applications of NLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer : Sentiment analysis, machine translation, chatbots, text classification, NER, summarization, and question-answering systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q26. What are stopwords and why are they removed in NLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer : Stopwords (e.g., \"the\", \"and\") are often removed to reduce noise and focus on meaningful words in tasks like text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q27. How can you implement word embeddings using Word2Vec in Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import Word2Vec\n",
    "# sentences = [[\"this\", \"is\", \"a\", \"sentence\"], ...]\n",
    "# model = Word2Vec(sentences, vector_size=100, window=5, min_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q28. How does SpaCy handle lemmatization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer : Uses vocabulary and morphological analysis to find lemmas (e.g., \"running\" → \"run\"), requiring POS tags for accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q29. What is the significance of RNNs in NLP tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer : RNNs process sequential data (text, time-series) by maintaining a memory of past inputs, enabling tasks like language modeling and translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q30. How does word embedding improve the performance of NLP models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer : Word embeddings (e.g., Word2Vec, GloVe) represent words as dense vectors, capturing semantic meaning, which improves model performance compared to sparse BoW representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q31. How does a Stacked LSTM differ from a single LSTM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer : A Stacked LSTM and a single LSTM differ primarily in the depth of the model architecture. The key difference is that a Stacked LSTM consists of multiple LSTM layers stacked on top of each other, whereas a single LSTM has just one layer. Here’s a more detailed breakdown:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q32. What are the key differences between RNN, LSTM, and GRU?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer : \n",
    "RNN: Struggles with long-term dependencies.\n",
    "\n",
    "LSTM: Uses gates to retain long-term context.\n",
    "\n",
    "GRU: Simplifies LSTM’s gates for faster computation but may trade some performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q33. Why is the attention mechanism important in sequence-to-sequence models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer : Attention allows the model to weigh relevant parts of the input sequence when generating outputs (e.g., translation), improving focus on critical information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q34. How would you implement text preprocessing using NLTK?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer : Text preprocessing is an essential step in Natural Language Processing (NLP) to clean and prepare raw text for modeling. NLTK (Natural Language Toolkit) provides a range of tools and utilities for common text preprocessing tasks, such as tokenization, stopword removal, stemming, lemmatization, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  How do you perform word tokenization using NLTK and plot a word frequency distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample text\n",
    "text = \"Word tokenization is the process of splitting text into individual words or tokens.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Calculate word frequency distribution\n",
    "fdist = FreqDist(tokens)\n",
    "\n",
    "# Plot the word frequency distribution\n",
    "fdist.plot(30, cumulative=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.  How do you use SpaCy for dependency parsing of a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "from spacy import load\n",
    "\n",
    "# Load the SpaCy model\n",
    "nlp = load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample sentence\n",
    "sentence = \"SpaCy is a powerful NLP library.\"\n",
    "\n",
    "# Parse the sentence\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# Print the dependencies\n",
    "for token in doc:\n",
    "    print(f\"{token.text} -> {token.dep_} -> {token.head.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. How do you use TextBlob for performing text classification based on polarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "# Sample text\n",
    "text = \"TextBlob is amazing for simple text processing tasks.\"\n",
    "\n",
    "# Create a TextBlob object\n",
    "blob = TextBlob(text)\n",
    "\n",
    "# Get the polarity\n",
    "polarity = blob.sentiment.polarity\n",
    "print(f\"Polarity: {polarity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. How do you extract named entities from a text using SpaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the SpaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample text\n",
    "text = \"Apple is looking at buying U.K. startup for $1 billion.\"\n",
    "\n",
    "# Parse the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract named entities\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text} -> {ent.label_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. How can you calculate TF-IDF scores for a given text using Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.37420726 0.         0.         0.\n",
      "  0.49203758 0.37420726 0.37420726 0.58121064]\n",
      " [0.         0.         0.         0.37420726 0.         0.49203758\n",
      "  0.         0.37420726 0.37420726 0.58121064]\n",
      " [0.42439575 0.42439575 0.32276391 0.32276391 0.42439575 0.\n",
      "  0.         0.         0.         0.50130994]]\n",
      "['and' 'are' 'cat' 'dog' 'friends' 'log' 'mat' 'on' 'sat' 'the']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"The dog sat on the log.\",\n",
    "    \"The cat and the dog are friends.\"\n",
    "]\n",
    "\n",
    "# Create the TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the documents\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Print the TF-IDF scores\n",
    "print(tfidf_matrix.toarray())\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. How do you create a custom text classifier using NLTK's Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\HEART\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.942\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.classify.util import accuracy as nltk_accuracy\n",
    "\n",
    "# Prepare the data\n",
    "nltk.download('movie_reviews')\n",
    "\n",
    "def extract_features(words):\n",
    "    return {word: True for word in words}\n",
    "\n",
    "# Load the movie reviews data\n",
    "reviews = [(list(movie_reviews.words(fileid)), category)\n",
    "           for category in movie_reviews.categories()\n",
    "           for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "# Extract features\n",
    "features = [(extract_features(review), category) for (review, category) in reviews]\n",
    "\n",
    "# Split the data\n",
    "train_set, test_set = features[:1500], features[1500:]\n",
    "\n",
    "# Train the classifier\n",
    "classifier = NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = nltk_accuracy(classifier, test_set)\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. How do you use a pre-trained model from Hugging Face for text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the text classification pipeline\n",
    "classifier = pipeline(\"text-classification\")\n",
    "\n",
    "# Sample text\n",
    "text = \"I love using Hugging Face transformers!\"\n",
    "\n",
    "# Perform text classification\n",
    "result = classifier(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. How do you perform text summarization using Hugging Face transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the summarization pipeline\n",
    "summarizer = pipeline(\"summarization\")\n",
    "\n",
    "# Sample text\n",
    "text = \"\"\"\n",
    "Transformers are a type of model architecture introduced in a paper\n",
    "titled \"Attention is All You Need\". They are designed to handle sequential\n",
    "data like text and have been used for various NLP tasks.\n",
    "\"\"\"\n",
    "\n",
    "# Perform text summarization\n",
    "summary = summarizer(text, max_length=50, min_length=25, do_sample=False)\n",
    "print(summary[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.  How can you create a simple RNN for text classification using Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HEART\\anaconda3\\envs\\NLP\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - accuracy: 1.0000 - loss: 0.6732\n",
      "Epoch 2/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 1.0000 - loss: 0.6397\n",
      "Epoch 3/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 1.0000 - loss: 0.6073\n",
      "Epoch 4/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 1.0000 - loss: 0.5760\n",
      "Epoch 5/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 1.0000 - loss: 0.5457\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1a830e61d30>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "texts = [\"I love machine learning\", \"I hate bugs in my code\"]\n",
    "labels = np.array([1, 0])  # Ensure labels are a numpy array\n",
    "\n",
    "# Tokenize the texts\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Pad the sequences\n",
    "padded_sequences = pad_sequences(sequences, padding='post')\n",
    "\n",
    "# Create the RNN model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=1000, output_dim=32, input_length=padded_sequences.shape[1]),\n",
    "    SimpleRNN(32),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(padded_sequences, labels, epochs=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. How do you train a Bidirectional LSTM for text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8s/step - accuracy: 0.0000e+00 - loss: 0.6966\n",
      "Epoch 2/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.5000 - loss: 0.6937\n",
      "Epoch 3/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 1.0000 - loss: 0.6908\n",
      "Epoch 4/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 1.0000 - loss: 0.6880\n",
      "Epoch 5/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 1.0000 - loss: 0.6852\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1a82b77e6d0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "texts = [\"I love machine learning\", \"I hate bugs in my code\"]\n",
    "labels = np.array([1, 0])  # Ensure labels are a numpy array\n",
    "\n",
    "# Tokenize the texts\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Pad the sequences\n",
    "padded_sequences = pad_sequences(sequences, padding='post')\n",
    "\n",
    "# Create the Bidirectional LSTM model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=1000, output_dim=32, input_length=padded_sequences.shape[1]),\n",
    "    Bidirectional(LSTM(32)),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(padded_sequences, labels, epochs=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. How do you implement GRU (Gated Recurrent Unit) for text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6s/step - accuracy: 1.0000 - loss: 0.6867\n",
      "Epoch 2/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 1.0000 - loss: 0.6817\n",
      "Epoch 3/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 1.0000 - loss: 0.6766\n",
      "Epoch 4/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 1.0000 - loss: 0.6715\n",
      "Epoch 5/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 1.0000 - loss: 0.6662\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1a82a2df790>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "texts = [\"I love machine learning\", \"I hate bugs in my code\"]\n",
    "labels = np.array([1, 0])  # Ensure labels are a numpy array\n",
    "\n",
    "# Tokenize the texts\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Pad the sequences\n",
    "padded_sequences = pad_sequences(sequences, padding='post')\n",
    "\n",
    "# Create the GRU model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=1000, output_dim=32, input_length=padded_sequences.shape[1]),\n",
    "    GRU(32),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(padded_sequences, labels, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12.  How do you implement a text generation model using LSTM with Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "texts = [\"I love machine learning\", \"Machine learning is fascinating\"]\n",
    "\n",
    "# Tokenize the texts\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Pad the sequences\n",
    "padded_sequences = pad_sequences(sequences, padding='post')\n",
    "\n",
    "# Create the LSTM model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=1000, output_dim=32, input_length=padded_sequences.shape[1]),\n",
    "    LSTM(32, return_sequences=True),\n",
    "    Dense(1000, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "# Train the model\n",
    "# Note: This is a simplified example. Text generation typically requires more complex preprocessing and training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13.  How do you implement a simple Bi-directional GRU for sequence labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10s/step - accuracy: 0.2500 - loss: 1.6126\n",
      "Epoch 2/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.3750 - loss: 1.6064\n",
      "Epoch 3/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.2500 - loss: 1.6002\n",
      "Epoch 4/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.3750 - loss: 1.5940\n",
      "Epoch 5/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.3750 - loss: 1.5878\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1a823cfd520>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, GRU, Dense, TimeDistributed\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Sample data\n",
    "texts = [\"I love machine learning\", \"Machine learning is fascinating\"]\n",
    "labels = [[1, 2, 3, 4], [1, 2, 3, 4]]  # Example labels for sequence labeling\n",
    "\n",
    "# Tokenize the texts\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Pad the sequences\n",
    "padded_sequences = pad_sequences(sequences, padding='post')\n",
    "\n",
    "# Assuming 5 possible labels, convert labels to categorical\n",
    "categorical_labels = [to_categorical(label, num_classes=5) for label in labels]\n",
    "padded_labels = pad_sequences(categorical_labels, padding='post', dtype='float32')\n",
    "\n",
    "# Create the Bi-directional GRU model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=1000, output_dim=32, input_length=padded_sequences.shape[1]),\n",
    "    Bidirectional(GRU(32, return_sequences=True)),\n",
    "    TimeDistributed(Dense(5, activation='softmax'))  # Assuming 5 possible labels\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(padded_sequences, padded_labels, epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.19.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
