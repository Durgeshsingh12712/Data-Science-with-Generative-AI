{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bb81b0d",
   "metadata": {},
   "source": [
    "# Assignment : Generative AI for Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d043c32f",
   "metadata": {},
   "source": [
    "Q1. What is Statistical Machine Translation (SMT)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d85d0e",
   "metadata": {},
   "source": [
    "Answer : Statistical Machine Translation (SMT) is a machine translation approach that uses statistical models to generate translations. It's based on the idea that a translation can be generated by analyzing large amounts of bilingual text data and using probability to determine the most likely translation. SMT systems typically consist of three components: a language model, a translation model, and a decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c15a1a",
   "metadata": {},
   "source": [
    "Q2. What are the main differences between SMT and Neural Machine Translation (NMT)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d5c2f6",
   "metadata": {},
   "source": [
    "Answer : The primary difference between SMT and NMT is the approach used to generate translations. SMT relies on statistical models and probability, whereas NMT uses deep learning techniques, such as neural networks, to learn the patterns and relationships between languages. NMT has become the dominant approach in machine translation due to its ability to handle complex linguistic structures and generate more fluent translations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28776d15",
   "metadata": {},
   "source": [
    "Q3. Explain the concept of attention in Neural Machine Translation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3034e0c",
   "metadata": {},
   "source": [
    "Answer : Attention is a mechanism used in NMT to focus on specific parts of the input sequence when generating a translation. It allows the model to selectively concentrate on certain words or phrases in the input sequence, rather than relying solely on the entire sequence. This helps the model to better capture long-range dependencies and generate more accurate translations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfa4aac",
   "metadata": {},
   "source": [
    "Q4. How do Generative Pre-trained Transformers (GPTs) contribute to machine translation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c748299d",
   "metadata": {},
   "source": [
    "Answer : GPTs are pre-trained language models that can be fine-tuned for specific tasks, including machine translation. They contribute to machine translation by providing a powerful and flexible framework for generating translations. GPTs can learn to capture the nuances of language and generate more fluent and natural-sounding translations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0180f0a8",
   "metadata": {},
   "source": [
    "Q5. What is poetry generation in generative AI?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8e3b34",
   "metadata": {},
   "source": [
    "Answer : Poetry generation is a subfield of generative AI that focuses on creating poetry using algorithms and machine learning techniques. It involves training models on large datasets of poetry and using various techniques, such as language models and generative adversarial networks, to generate new poems that mimic the style and structure of the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45742af",
   "metadata": {},
   "source": [
    "Q6. How does music composition with generative AI work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b1dc67",
   "metadata": {},
   "source": [
    "Answer : Music composition with generative AI involves using algorithms and machine learning techniques to generate music. This can be done using various approaches, such as generating musical patterns, melodies, or harmonies, or even entire compositions. Generative AI models can be trained on large datasets of music and can learn to capture the styles and structures of different genres and composers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e86e1c2",
   "metadata": {},
   "source": [
    "Q7. What role does reinforcement learning play in generative AI for NLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293e35cf",
   "metadata": {},
   "source": [
    "Answer : Reinforcement learning is a type of machine learning that involves training models to make decisions based on rewards or penalties. In generative AI for NLP, reinforcement learning can be used to train models to generate text that is more coherent, fluent, or engaging. It can also be used to fine-tune models to specific tasks or styles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdb7473",
   "metadata": {},
   "source": [
    "Q8. What are multimodal generative models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5412447",
   "metadata": {},
   "source": [
    "Answer : Multimodal generative models are AI models that can generate multiple forms of data, such as text, images, or music. These models can learn to capture the relationships between different modalities and generate new data that is consistent across multiple forms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5a2a68",
   "metadata": {},
   "source": [
    "Q9. Define Natural Language Understanding (NLU) in the context of generative AI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e805caa",
   "metadata": {},
   "source": [
    "Answer : Natural Language Understanding (NLU) refers to the ability of AI models to comprehend and interpret human language. In the context of generative AI, NLU is critical for generating text that is coherent, fluent, and relevant to the context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd99cfe",
   "metadata": {},
   "source": [
    "Q10. What ethical considerations arise in generative AI for creative writing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87141920",
   "metadata": {},
   "source": [
    "Answer : Ethical considerations in generative AI for creative writing include issues of authorship, ownership, and bias. There are concerns about the potential for AI-generated content to displace human writers, as well as the risk of perpetuating biases and stereotypes in the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9bbc9d",
   "metadata": {},
   "source": [
    "Q11. How can attention mechanisms improve NMT performance on longer sentences?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077ae7df",
   "metadata": {},
   "source": [
    "Answer : Attention mechanisms can improve NMT performance on longer sentences by allowing the model to selectively focus on specific parts of the input sequence. This helps the model to better capture long-range dependencies and generate more accurate translations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f169d23c",
   "metadata": {},
   "source": [
    "Q12. What are some challenges with bias in generative AI for machine translation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9790bf01",
   "metadata": {},
   "source": [
    "Answer : Challenges with bias in generative AI for machine translation include the risk of perpetuating biases and stereotypes in the training data, as well as the potential for cultural insensitivity. There is also a risk of overfitting to specific dialects or accents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa89df49",
   "metadata": {},
   "source": [
    "Q13. What is the role of a decoder in NMT models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296ab85c",
   "metadata": {},
   "source": [
    "Answer : The decoder is a critical component of NMT models, responsible for generating the target language translation. It takes the output from the encoder and generates a sequence of words that form the translation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e805375",
   "metadata": {},
   "source": [
    "Q14. Explain how reinforcement learning differs from supervised learning in generative AI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5887bac4",
   "metadata": {},
   "source": [
    "Answer : Reinforcement learning differs from supervised learning in that it involves training models to make decisions based on rewards or penalties, rather than relying on labeled data. This allows models to learn from feedback and adapt to new situations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d047f1",
   "metadata": {},
   "source": [
    "Q15. How does fine-tuning a GPT model differ from pre-training it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9903dc",
   "metadata": {},
   "source": [
    "Answer : Fine-tuning a GPT model involves taking a pre-trained model and adjusting its weights to fit a specific task or dataset. This is different from pre-training, which involves training the model from scratch on a large dataset. Fine-tuning allows the model to adapt to a specific task or style, while pre-training provides the foundation for the model's language understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0218eb",
   "metadata": {},
   "source": [
    "Q16. Describe one approach generative AI uses to avoid overfitting in creative content generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ecbc8d",
   "metadata": {},
   "source": [
    "Answer : One approach generative AI uses to avoid overfitting is regularization techniques, such as dropout or weight decay. These techniques randomly remove or reduce the strength of certain connections in the model, preventing it from becoming too specialized to the training data. This helps the model to generalize better and generate more diverse and creative content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e4e39c",
   "metadata": {},
   "source": [
    "Q17. What makes GPT-based models effective for creative storytelling?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae43a6c",
   "metadata": {},
   "source": [
    "Answer : GPT-based models are effective for creative storytelling because they can learn to capture the patterns and structures of language, allowing them to generate coherent and engaging narratives. Their ability to process large amounts of text data and learn from context makes them well-suited for tasks like story generation, character development, and dialogue creation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba429d88",
   "metadata": {},
   "source": [
    "Q18. How does context preservation work in NMT models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a529cad4",
   "metadata": {},
   "source": [
    "Answer : Context preservation in NMT models involves using techniques like attention mechanisms and memory-augmented models to retain information about the input sequence and its context. This allows the model to generate translations that are more accurate and relevant to the original text, taking into account nuances like idioms, colloquialisms, and cultural references."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b11e9df",
   "metadata": {},
   "source": [
    "Q19. What is the main advantage of multimodal models in creative applications?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346bbe32",
   "metadata": {},
   "source": [
    "Answer : The main advantage of multimodal models in creative applications is their ability to combine and integrate different forms of data, such as text, images, and audio. This allows them to generate more rich and immersive experiences, like interactive stories, multimedia presentations, or even entire virtual worlds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52787062",
   "metadata": {},
   "source": [
    "Q20. How does generative AI handle cultural nuances in translation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417a9494",
   "metadata": {},
   "source": [
    "Answer : Generative AI can handle cultural nuances in translation by learning from large datasets that include diverse cultural references and contexts. However, this requires careful curation of the training data to ensure that the model is exposed to a wide range of cultural perspectives and expressions. Additionally, techniques like domain adaptation and transfer learning can help the model to adapt to specific cultural contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013ba38a",
   "metadata": {},
   "source": [
    "Q21. Why is it difficult to fully remove bias in generative AI models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5751889",
   "metadata": {},
   "source": [
    "Answer : It's difficult to fully remove bias in generative AI models because bias can be deeply ingrained in the training data, and models can learn to perpetuate and even amplify these biases. Additionally, bias can be subtle and implicit, making it challenging to detect and address. Furthermore, the complexity of human culture and society means that bias can manifest in many different ways, requiring ongoing effort and attention to mitigate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c752fa",
   "metadata": {},
   "source": [
    "# Practical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f68d769",
   "metadata": {},
   "source": [
    "1. Implement a basic Statistical Machine Translation (SMT) model that uses word-by-word translation with a dictionary lookup approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da1fb555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bonjour monde ce est un test\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define a dictionary for word-by-word translation\n",
    "dictionary = {\n",
    "    'hello': 'bonjour',\n",
    "    'world': 'monde',\n",
    "    'this': 'ce',\n",
    "    'is': 'est',\n",
    "    'a': 'un',\n",
    "    'test': 'test'\n",
    "}\n",
    "\n",
    "def smt_translate(sentence):\n",
    "    words = sentence.split()\n",
    "    translated_words = [dictionary.get(word, word) for word in words]\n",
    "    return ' '.join(translated_words)\n",
    "\n",
    "# Test the SMT model\n",
    "sentence = \"hello world this is a test\"\n",
    "print(smt_translate(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e7a52d",
   "metadata": {},
   "source": [
    "2. Implement an Attention mechanism in a Neural Machine Translation (NMT) model using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "616dbf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.W = nn.Linear(hidden_size, hidden_size)\n",
    "        self.U = nn.Linear(hidden_size, hidden_size)\n",
    "        self.w = nn.Parameter(torch.randn(hidden_size))\n",
    "\n",
    "    def forward(self, encoder_output, decoder_hidden):\n",
    "        weights = torch.tanh(self.W(encoder_output) + self.U(decoder_hidden))\n",
    "        weights = weights * self.w\n",
    "        weights = weights.sum(dim=2)\n",
    "        weights = torch.softmax(weights, dim=1)\n",
    "        return weights\n",
    "\n",
    "class NMTModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(NMTModel, self).__init__()\n",
    "        self.encoder = nn.GRU(input_size, hidden_size, num_layers=1, batch_first=True)\n",
    "        self.decoder = nn.GRU(hidden_size, hidden_size, num_layers=1, batch_first=True)\n",
    "        self.attention = Attention(hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        encoder_output, _ = self.encoder(input_seq)\n",
    "        decoder_hidden = encoder_output[:, -1, :]\n",
    "        weights = self.attention(encoder_output, decoder_hidden)\n",
    "        context_vector = weights * encoder_output\n",
    "        context_vector = context_vector.sum(dim=1)\n",
    "        output = self.fc(context_vector)\n",
    "        return output\n",
    "\n",
    "# Initialize the NMT model\n",
    "input_size = 128\n",
    "hidden_size = 256\n",
    "output_size = 128\n",
    "model = NMTModel(input_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d46c76e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMTModel(\n",
       "  (encoder): GRU(128, 256, batch_first=True)\n",
       "  (decoder): GRU(256, 256, batch_first=True)\n",
       "  (attention): Attention(\n",
       "    (W): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (U): Linear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (fc): Linear(in_features=256, out_features=128, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3629d415",
   "metadata": {},
   "source": [
    "3. Use a pre-trained GPT model to perform machine translation from English to French."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e0dd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "\n",
    "# Load pre-trained GPT model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "\n",
    "# Define a function for machine translation\n",
    "def translate_to_french(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    output = model.generate(input_ids, max_length=50)\n",
    "    output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return output_text\n",
    "\n",
    "# Test the machine translation function\n",
    "input_text = \"Hello, how are you?\"\n",
    "print(translate_to_french(input_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f368c0d4",
   "metadata": {},
   "source": [
    "4. Generate a short poem using GPT-2 for a specific theme (e.g., \"Nature\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5a88b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "\n",
    "# Load pre-trained GPT model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "\n",
    "# Define a function for generating a poem\n",
    "def generate_poem(theme):\n",
    "    input_ids = tokenizer.encode(theme, return_tensors='pt')\n",
    "    output = model.generate(input_ids, max_length=100)\n",
    "    output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return output_text\n",
    "\n",
    "# Test the poem generation function\n",
    "theme = \"Nature\"\n",
    "print(generate_poem(theme))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ecf068",
   "metadata": {},
   "source": [
    "5. Implement a basic reinforcement learning setup for text generation using PyTorch's reward function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5574fb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class TextGenerator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        output = torch.relu(self.fc1(input_seq))\n",
    "        output = self.fc2(output)\n",
    "        return output\n",
    "\n",
    "class RewardFunction(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RewardFunction, self).__init__()\n",
    "\n",
    "    def forward(self, output_seq):\n",
    "        # Define a reward function that encourages the model to generate coherent text\n",
    "        reward = 0\n",
    "        for i in range(len(output_seq) - 1):\n",
    "            if output_seq[i] == output_seq[i + 1]:\n",
    "                reward += 1\n",
    "        return reward\n",
    "\n",
    "# Initialize the text generator and reward function\n",
    "input_size = 128\n",
    "hidden_size = 256\n",
    "output_size = 128\n",
    "text_generator = TextGenerator(input_size, hidden_size, output_size)\n",
    "reward_function = RewardFunction()\n",
    "\n",
    "# Define a reinforcement learning loop\n",
    "def reinforcement_learning_loop():\n",
    "    optimizer = optim.Adam(text_generator.parameters(), lr=0.001)\n",
    "    for episode in range(1000):\n",
    "        input_seq = torch.randn(1, input_size)\n",
    "        output_seq = text_generator(input_seq)\n",
    "        reward = reward_function(output_seq)\n",
    "        loss = -reward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f'Episode {episode+1}, Reward: {reward.item()}')\n",
    "\n",
    "# Run the reinforcement learning loop\n",
    "reinforcement_learning_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f579e473",
   "metadata": {},
   "source": [
    "6. Create a simple multimodal generative model that generates an image caption given an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef618a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class ImageCaptionGenerator(nn.Module):\n",
    "    def __init__(self, image_size, hidden_size, output_size):\n",
    "        super(ImageCaptionGenerator, self).__init__()\n",
    "        self.image_encoder = torchvision.models.resnet50(pretrained=True)\n",
    "        self.image_encoder.fc = nn.Linear(512, hidden_size)\n",
    "        self.caption_generator = nn.LSTM(hidden_size, hidden_size, num_layers=1, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, image):\n",
    "        image_features = self.image_encoder(image)\n",
    "        caption = self.caption_generator(image_features.unsqueeze(1))\n",
    "        caption = self.fc(caption[0])\n",
    "        return caption\n",
    "\n",
    "# Initialize the image caption generator\n",
    "image_size = 224\n",
    "hidden_size = 256\n",
    "output_size = 128\n",
    "image_caption_generator = ImageCaptionGenerator(image_size, hidden_size, output_size)\n",
    "\n",
    "# Define a dataset and data loader for images and captions\n",
    "transform = transforms.Compose([transforms.Resize(image_size), transforms.CenterCrop(image_size), transforms.ToTensor()])\n",
    "dataset = torchvision.datasets.ImageFolder('path/to/images', transform)\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Train the image caption generator\n",
    "def train_image_caption_generator():\n",
    "    optimizer = optim.Adam(image_caption_generator.parameters(), lr=0.001)\n",
    "    for epoch in range(10):\n",
    "        for batch in data_loader:\n",
    "            images, _ = batch\n",
    "            captions = image_caption_generator(images)\n",
    "            loss = nn.CrossEntropyLoss()(captions, torch.zeros(captions.size(0), dtype=torch.long))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "# Train the image caption generator\n",
    "train_image_caption_generator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4873f1e9",
   "metadata": {},
   "source": [
    "7. Demonstrate how to evaluate bias in generated content by analyzing GPT responses to prompts with\n",
    "potentially sensitive terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110c748b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "\n",
    "# Load pre-trained GPT model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "\n",
    "# Define a function to generate responses to prompts\n",
    "def generate_response(prompt):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    output = model.generate(input_ids, max_length=50)\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Define a list of potentially sensitive terms\n",
    "sensitive_terms = ['race', 'gender', 'religion']\n",
    "\n",
    "# Analyze GPT responses to prompts with sensitive terms\n",
    "def analyze_bias():\n",
    "    for term in sensitive_terms:\n",
    "        prompt = f'What is your opinion on {term}?'\n",
    "        response = generate_response(prompt)\n",
    "        print(f'Prompt: {prompt}, Response: {response}')\n",
    "\n",
    "# Analyze bias in GPT responses\n",
    "analyze_bias()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540c7f54",
   "metadata": {},
   "source": [
    "8. Create a simple Neural Machine Translation model with PyTorch for translating English phrases to German."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e23265",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Define a dataset class for our English-German translation data\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, english_sentences, german_sentences):\n",
    "        self.english_sentences = english_sentences\n",
    "        self.german_sentences = german_sentences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.english_sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        english_sentence = self.english_sentences[idx]\n",
    "        german_sentence = self.german_sentences[idx]\n",
    "\n",
    "        # Convert sentences to tensors\n",
    "        english_tensor = torch.tensor([ord(c) for c in english_sentence])\n",
    "        german_tensor = torch.tensor([ord(c) for c in german_sentence])\n",
    "\n",
    "        return english_tensor, german_tensor\n",
    "\n",
    "# Load the English-German translation data\n",
    "english_sentences = [\"Hello, how are you?\", \"What is your name?\", \"I love to learn.\"]\n",
    "german_sentences = [\"Hallo, wie geht es dir?\", \"Wie hei√üt du?\", \"Ich liebe zu lernen.\"]\n",
    "\n",
    "# Create a dataset and data loader for our translation data\n",
    "dataset = TranslationDataset(english_sentences, german_sentences)\n",
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Define the NMT model\n",
    "class NMTModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(NMTModel, self).__init__()\n",
    "        self.encoder = nn.GRU(input_size, hidden_size, num_layers=1, batch_first=True)\n",
    "        self.decoder = nn.GRU(hidden_size, hidden_size, num_layers=1, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        encoder_output, _ = self.encoder(input_seq)\n",
    "        decoder_output, _ = self.decoder(encoder_output)\n",
    "        output = self.fc(decoder_output[:, -1, :])\n",
    "        return output\n",
    "\n",
    "# Initialize the NMT model\n",
    "input_size = 256\n",
    "hidden_size = 256\n",
    "output_size = 256\n",
    "model = NMTModel(input_size, hidden_size, output_size)\n",
    "\n",
    "# Define a loss function and optimizer for training the model\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the NMT model\n",
    "def train_model():\n",
    "    for epoch in range(10):\n",
    "        for batch in data_loader:\n",
    "            english_tensors, german_tensors = batch\n",
    "            english_tensors = english_tensors.view(-1, 1, input_size)\n",
    "            german_tensors = german_tensors.view(-1, 1, output_size)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(english_tensors)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = criterion(outputs, german_tensors[:, 0, 0])\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the model parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "# Train the model\n",
    "train_model()\n",
    "\n",
    "# Use the trained model to translate English phrases to German\n",
    "def translate_to_german(english_phrase):\n",
    "    english_tensor = torch.tensor([ord(c) for c in english_phrase])\n",
    "    english_tensor = english_tensor.view(1, -1, input_size)\n",
    "    output = model(english_tensor)\n",
    "    german_phrase = ''.join([chr(torch.argmax(output).item())])\n",
    "    return german_phrase\n",
    "\n",
    "# Test the translation function\n",
    "english_phrase = \"Hello, how are you?\"\n",
    "german_phrase = translate_to_german(english_phrase)\n",
    "print(f'English: {english_phrase}, German: {german_phrase}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0a4154",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
